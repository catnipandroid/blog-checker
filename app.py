import os
import json
from io import BytesIO

import streamlit as st
from docx import Document
from docx.enum.text import WD_COLOR_INDEX
from docx.shared import RGBColor
from openai import OpenAI
import streamlit_authenticator as stauth

# =========================
# OpenAI ì„¤ì •
# =========================

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
client = None
if OPENAI_API_KEY:
    client = OpenAI(api_key=OPENAI_API_KEY)


# =========================
# ë¡œê·¸ì¸ ì„¤ì • (ì—¬ê¸° í•´ì‹œê°’/ì´ë©”ì¼ë§Œ ë„¤ ê±¸ë¡œ ë°”ê¾¸ë©´ ë¨)
# =========================

credentials = {
    "usernames": {
        "jaehyun": {
            "name": "ì¬í˜„",
            "password": "$2b$12$ì—¬ê¸°ì—_bcrypt_í•´ì‹œ_ë¶™ì—¬ë„£ê¸°",  # make_hash.py ê²°ê³¼
            "email": "jaehyun@example.com",
            "roles": ["admin"],
        },
        # íŒ€ì› ì¶”ê°€ ì˜ˆì‹œ
        # "member1": {
        #     "name": "íŒ€ì›1",
        #     "password": "$2b$12$íŒ€ì›_í•´ì‹œê°’",
        #     "email": "member1@example.com",
        #     "roles": ["user"],
        # },
    }
}

authenticator = stauth.Authenticate(
    credentials,
    "blog_checker_cookie",   # cookie ì´ë¦„
    "some_random_key_123",   # ì‹œí¬ë¦¿ í‚¤ (ì„ì˜ ë¬¸ìì—´)
    1,                       # ì¿ í‚¤ ë§Œë£Œì¼ (ì¼)
)


# =========================
# ê³µí†µ ìœ í‹¸
# =========================

def highlight_paragraph(paragraph, color=WD_COLOR_INDEX.YELLOW):
    for run in paragraph.runs:
        run.font.highlight_color = color


def add_comment_below(doc: Document, paragraph, comment_text: str):
    new_para = doc.add_paragraph()
    run = new_para.add_run(f"[ìë™ê²€ìˆ˜] {comment_text}")
    run.bold = True
    run.font.color.rgb = RGBColor(0xFF, 0x00, 0x00)
    for r in new_para.runs:
        r.font.highlight_color = WD_COLOR_INDEX.YELLOW
    paragraph._p.addnext(new_para._p)


def get_full_text(doc: Document) -> str:
    return "\n".join(p.text for p in doc.paragraphs)


# =========================
# 1ë‹¨ê³„: ë£° ê¸°ë°˜ ì²´í¬
# =========================

def check_utm_links(doc: Document, report: list):
    """http ë“¤ì–´ê°”ëŠ”ë° utm_ ì—†ëŠ” ë§í¬"""
    count = 0
    for para in doc.paragraphs:
        text = para.text
        if "http" in text and "utm_" not in text:
            highlight_paragraph(para)
            add_comment_below(doc, para, "UTM íŒŒë¼ë¯¸í„°ê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤. (ì˜ˆ: ?utm_source=...)")
            count += 1

    if count > 0:
        report.append(f"- [ë£°] UTM ëˆ„ë½ ë¬¸ë‹¨ {count}ê°œ")
    else:
        report.append("- [ë£°] UTM ê´€ë ¨ ë¬¸ì œ ì—†ìŒ")


def check_hashtags(doc: Document, report: list, config: dict):
    """ê¶Œì¥ í•´ì‹œíƒœê·¸ í¬í•¨ ì—¬ë¶€"""
    recommended = config["recommended_hashtags"]
    full_text = get_full_text(doc)
    missing = [t for t in recommended if t and t not in full_text]

    if missing:
        p = doc.add_paragraph()
        run = p.add_run("[ìë™ê²€ìˆ˜] ì•„ë˜ í•´ì‹œíƒœê·¸ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤: " + ", ".join(missing))
        run.bold = True
        run.font.color.rgb = RGBColor(0xFF, 0x00, 0x00)
        for r in p.runs:
            r.font.highlight_color = WD_COLOR_INDEX.YELLOW

        report.append(f"- [ë£°] í•´ì‹œíƒœê·¸ ë¶€ì¡±: {len(missing)}ê°œ (ê¶Œì¥ í•´ì‹œíƒœê·¸ ì¼ë¶€ ëˆ„ë½)")
    else:
        report.append("- [ë£°] í•´ì‹œíƒœê·¸ ëª¨ë‘ í¬í•¨ë¨")


def check_shopby(doc: Document, report: list, config: dict):
    """ìƒµë°”ì´ ì–¸ê¸‰ ë¬¸ë‹¨"""
    shopby_keywords = config["shopby_keywords"]
    count = 0
    for para in doc.paragraphs:
        text = para.text
        if any(keyword.lower() in text.lower() for keyword in shopby_keywords):
            highlight_paragraph(para)
            add_comment_below(doc, para, "ìƒµë°”ì´(Shopby) ê´€ë ¨ ë‚´ìš©ì€ ë¸”ë¡œê·¸ì— í¬í•¨ë  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            count += 1

    if count > 0:
        report.append(f"- [ë£°] ìƒµë°”ì´ ì–¸ê¸‰ ë¬¸ë‹¨ {count}ê°œ")
    else:
        report.append("- [ë£°] ìƒµë°”ì´ ì–¸ê¸‰ ì—†ìŒ")


def check_b2b_basic_feature(doc: Document, report: list, config: dict):
    """B2B + ê¸°ë³¸ ê¸°ëŠ¥ ë‰˜ì•™ìŠ¤"""
    b2b_keywords = config["b2b_keywords"]
    basic_keywords = config["basic_feature_keywords"]

    count = 0
    for para in doc.paragraphs:
        text = para.text
        if any(b in text for b in b2b_keywords) and any(k in text for k in basic_keywords):
            highlight_paragraph(para)
            add_comment_below(doc, para, "B2B ê¸°ëŠ¥ì´ ê¸°ë³¸ ì œê³µëœë‹¤ëŠ” ì˜¤í•´ë¥¼ ì¤„ ìˆ˜ ìˆëŠ” í‘œí˜„ì…ë‹ˆë‹¤.")
            count += 1

    if count > 0:
        report.append(f"- [ë£°] B2Bë¥¼ ê¸°ë³¸ ê¸°ëŠ¥ì²˜ëŸ¼ í‘œí˜„í•œ ë¬¸ë‹¨ {count}ê°œ")
    else:
        report.append("- [ë£°] B2B ê¸°ë³¸ ê¸°ëŠ¥ ì˜¤í•´ í‘œí˜„ ì—†ìŒ")


def check_haedream(doc: Document, report: list, config: dict):
    """í•´ë“œë¦¼ ì–¸ê¸‰ ë¬¸ë‹¨"""
    haedream_keywords = config["haedream_keywords"]
    count = 0
    for para in doc.paragraphs:
        text = para.text
        if any(k in text for k in haedream_keywords):
            highlight_paragraph(para)
            add_comment_below(doc, para, "í•´ë“œë¦¼ í‘œê¸° ë°©ì‹ì´ ì •ì±…ì— ë§ëŠ”ì§€ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            count += 1

    if count > 0:
        report.append(f"- [ë£°] í•´ë“œë¦¼ ì–¸ê¸‰ ë¬¸ë‹¨ {count}ê°œ")
    else:
        report.append("- [ë£°] í•´ë“œë¦¼ ì–¸ê¸‰ ì—†ìŒ")


def check_media_count(doc: Document, report: list, min_images: int = 15):
    """ì´ë¯¸ì§€ ê°œìˆ˜ / ì˜ìƒ URL ì—¬ë¶€"""
    img_count = len(doc.inline_shapes)
    full_text = get_full_text(doc)

    # ì•„ì£¼ ëŸ¬í”„í•˜ê²Œ ì˜ìƒ URL ì²´í¬
    has_video = any(k in full_text for k in ["youtube.com", "youtu.be", "vimeo.com", "video"])

    # ì´ë¯¸ì§€ ê°œìˆ˜
    if img_count < min_images:
        p = doc.add_paragraph()
        add_comment_below(
            doc,
            p,
            f"ì´ë¯¸ì§€ ê°œìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (í˜„ì¬ {img_count}ì¥ / ê¸°ì¤€ {min_images}ì¥ ì´ìƒ)"
        )
        report.append(f"- [ë£°] ì´ë¯¸ì§€ ê°œìˆ˜ ë¶€ì¡±: {img_count}ì¥ (ê¸°ì¤€ {min_images}ì¥)")
    else:
        report.append(f"- [ë£°] ì´ë¯¸ì§€ ê°œìˆ˜ ì¶©ì¡±: {img_count}ì¥")

    # ì˜ìƒ
    if not has_video:
        report.append("- [ë£°] ë™ì˜ìƒ ì‚½ì… ì—†ìŒ (ì˜ìƒ 1ê°œ ì´ìƒ ê¶Œì¥)")
    else:
        report.append("- [ë£°] ë™ì˜ìƒ URL í¬í•¨ë¨ (youtube ë“±)")


def check_forbidden_terms(doc: Document, report: list, client_brands: list[str], competitors: list[str]):
    """ê³ ê°ì‚¬ ë¸Œëœë“œ / íƒ€ì‚¬ ê¸ˆì§€ì–´"""
    client_count = 0
    comp_count = 0

    for para in doc.paragraphs:
        text_lower = para.text.lower()
        if any(b.lower() in text_lower for b in client_brands):
            highlight_paragraph(para)
            add_comment_below(doc, para, "ê³ ê°ì‚¬ ë¸Œëœë“œëª… ì–¸ê¸‰ ê¸ˆì§€ ëŒ€ìƒì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            client_count += 1

        if any(c.lower() in text_lower for c in competitors):
            highlight_paragraph(para)
            add_comment_below(doc, para, "íƒ€ì‚¬(ê²½ìŸì‚¬) ì–¸ê¸‰ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
            comp_count += 1

    report.append(f"- [ë£°] ê³ ê°ì‚¬ ë¸Œëœë“œ ì–¸ê¸‰ ë¬¸ë‹¨: {client_count}ê°œ")
    report.append(f"- [ë£°] íƒ€ì‚¬/ê²½ìŸì‚¬ ì–¸ê¸‰ ë¬¸ë‹¨: {comp_count}ê°œ")


def check_avoided_phrases(doc: Document, report: list, avoided_phrases: list[str]):
    """ì‡¼í•‘ëª°í˜¸ìŠ¤íŒ…ì‚¬, ì „ììƒê±°ë˜ í”Œë«í¼, ë°˜ì‘í˜•ìŠ¤í‚¨ ë“± ì§€ì–‘ í‘œí˜„"""
    count = 0
    for para in doc.paragraphs:
        text_lower = para.text.lower()
        if any(p.lower() in text_lower for p in avoided_phrases):
            highlight_paragraph(para)
            add_comment_below(doc, para, "ë‚´ë¶€ì—ì„œ ì§€ì–‘í•˜ëŠ” í‘œí˜„ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ë¬¸êµ¬ ìˆ˜ì • í•„ìš”.")
            count += 1

    report.append(f"- [ë£°] ì§€ì–‘ í‘œí˜„ì´ í¬í•¨ëœ ë¬¸ë‹¨: {count}ê°œ")


def check_title_keyword(doc: Document, report: list, required_keyword: str | None):
    """ì œëª©ì— í•„ìˆ˜ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€"""
    if not required_keyword:
        report.append("- [ë£°] ì œëª© í‚¤ì›Œë“œ ê¸°ì¤€ ë¯¸ì„¤ì • (ìˆ˜ë™ ì²´í¬)")
        return

    if not doc.paragraphs:
        report.append("- [ë£°] ë¬¸ë‹¨ì´ ì—†ì–´ ì œëª©ì„ í™•ì¸í•  ìˆ˜ ì—†ìŒ")
        return

    title_para = doc.paragraphs[0]
    if required_keyword not in title_para.text:
        highlight_paragraph(title_para)
        add_comment_below(
            doc,
            title_para,
            f"ì œëª©ì— ì§€ì •ëœ í‚¤ì›Œë“œ('{required_keyword}')ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤."
        )
        report.append("- [ë£°] ì œëª© í‚¤ì›Œë“œ ë¯¸í¬í•¨")
    else:
        report.append("- [ë£°] ì œëª©ì— ì§€ì • í‚¤ì›Œë“œ í¬í•¨")


# =========================
# 2ë‹¨ê³„: LLM ê¸°ë°˜ ì²´í¬ (ì˜µì…˜)
# =========================

def analyze_paragraph_with_llm(text: str) -> dict | None:
    if not client:
        return None
    if not text.strip():
        return None

    prompt = f"""
ë„ˆëŠ” NHNì»¤ë¨¸ìŠ¤ ê³ ë„ëª° ë¸”ë¡œê·¸ ì›ê³ ë¥¼ ê²€ìˆ˜í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ë‹¤.

ì•„ë˜ ë¬¸ë‹¨ì„ ë³´ê³  ë‹¤ìŒ í•­ëª©ë“¤ì„ íŒë‹¨í•´ë¼.
ë°˜ë“œì‹œ JSON ë¬¸ìì—´ë§Œ ì¶œë ¥í•˜ë¼.

ê·œì¹™:
1) "b2b_as_basic":    B2B ê¸°ëŠ¥ì´ ê¸°ë³¸ ê¸°ëŠ¥ì²˜ëŸ¼ ë³´ì´ê²Œ í‘œí˜„ëëŠ”ì§€ ì—¬ë¶€.
2) "free_b2b_mix":    ë¬´ë£Œ/0ì› í”„ë¡œëª¨ì…˜ + B2B ë‚´ìš©ì´ ì„ì—¬ ì˜ëª»ëœ ë‰˜ì•™ìŠ¤ë¥¼ ì£¼ëŠ”ì§€ ì—¬ë¶€.
3) "haedream_mislabel":  í•´ë“œë¦¼ì„ ê³µì‹ ì—ì´ì „ì‹œì²˜ëŸ¼ ì˜ëª» í‘œê¸°í–ˆëŠ”ì§€ ì—¬ë¶€.
4) "typo_exists":     ë§ì¶¤ë²•/ë„ì–´ì“°ê¸° ë¬¸ì œê°€ ìˆëŠ”ì§€ ì—¬ë¶€.
5) "typo_examples":   ëŒ€í‘œì  ë§ì¶¤ë²• ì˜¤ë¥˜ ë‹¨ì–´ 3ê°œ ì´í•˜.

ì¶œë ¥ í˜•ì‹(JSON ì˜ˆì‹œ):

{{
  "b2b_as_basic": false,
  "free_b2b_mix": true,
  "haedream_mislabel": false,
  "typo_exists": true,
  "typo_examples": ["ì˜ˆì‹œ1", "ì˜ˆì‹œ2"]
}}

ê²€ìˆ˜í•  ë¬¸ë‹¨:
\"\"\"{text}\"\"\"
"""

    try:
        resp = client.responses.create(
            model="gpt-4.1-mini",
            input=prompt,
            timeout=20,
        )
    except Exception as e:
        print("[LLM ì˜¤ë¥˜] ìš”ì²­ ì¤‘ ì˜ˆì™¸ ë°œìƒ:", e)
        return None

    content = resp.output_text
    try:
        data = json.loads(content)
        return data
    except Exception:
        print("[LLM] JSON íŒŒì‹± ì‹¤íŒ¨. ì‘ë‹µ:", content[:200], "...")
        return None


def check_with_llm(doc: Document, report: list, config: dict, use_llm: bool):
    if not use_llm or not client:
        if not client:
            report.append("- [LLM] OPENAI_API_KEY ë¯¸ì„¤ì •ìœ¼ë¡œ LLM ê²€ìˆ˜ëŠ” ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            report.append("- [LLM] LLM ê²€ìˆ˜ ì˜µì…˜ì´ êº¼ì ¸ ìˆìŠµë‹ˆë‹¤.")
        return

    suspicious_keywords = config["suspicious_keywords"]

    b2b_basic_count = 0
    free_b2b_mix_count = 0
    haedream_mislabel_count = 0
    typo_count = 0

    paragraphs = list(doc.paragraphs)
    total = len(paragraphs)

    for idx, para in enumerate(paragraphs):
        text = para.text.strip()
        if not text or len(text) < 15:
            continue

        lower = text.lower()
        if not any(k.lower() in lower for k in suspicious_keywords):
            continue

        print(f"[LLM] {idx+1}/{total} ë¬¸ë‹¨ ê²€ì‚¬ ì¤‘...")

        result = analyze_paragraph_with_llm(text)
        if not result:
            continue

        if result.get("b2b_as_basic"):
            highlight_paragraph(para)
            add_comment_below(
                doc,
                para,
                "LLM: B2B ê¸°ëŠ¥ì´ 'ê¸°ë³¸ ì œê³µ'ì²˜ëŸ¼ ë³´ì´ëŠ” í‘œí˜„ì…ë‹ˆë‹¤. "
                "ì»¤ìŠ¤í„°ë§ˆì´ì§•ì´ í•„ìš”í•˜ë‹¤ëŠ” ì ì„ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤."
            )
            b2b_basic_count += 1

        if result.get("free_b2b_mix"):
            highlight_paragraph(para)
            add_comment_below(
                doc,
                para,
                "LLM: ë¬´ë£Œ/0ì› í”„ë¡œëª¨ì…˜ê³¼ B2B íŠœë‹ ë‚´ìš©ì´ ì„ì—¬, "
                "B2Bë„ ë¬´ë£Œë¡œ ì‹œì‘ ê°€ëŠ¥í•œ ê²ƒì²˜ëŸ¼ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            )
            free_b2b_mix_count += 1

        if result.get("haedream_mislabel"):
            highlight_paragraph(para)
            add_comment_below(
                doc,
                para,
                "LLM: í•´ë“œë¦¼ì„ ê³µì‹ ì—ì´ì „ì‹œ/ì œì‘ ëŒ€í–‰ì‚¬ì²˜ëŸ¼ í‘œí˜„í•œ ë¶€ë¶„ì´ ìˆìŠµë‹ˆë‹¤. "
                "â€˜ë§ì¶¤ ì œì‘ ìƒë‹´ì„ í†µí•´ ê³µì‹ ì—ì´ì „ì‹œë¥¼ ì—°ê²°â€™í•˜ëŠ” ì—­í• ë¡œ í‘œì‹œí•´ì•¼ í•©ë‹ˆë‹¤."
            )
            haedream_mislabel_count += 1

        if result.get("typo_exists"):
            examples = result.get("typo_examples") or []
            example_text = ", ".join(examples) if examples else "ëŒ€í‘œì ì¸ ì˜¤ë¥˜ ì˜ˆì‹œë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."
            add_comment_below(
                doc,
                para,
                f"LLM: ì´ ë¬¸ë‹¨ì— ë§ì¶¤ë²•/ë„ì–´ì“°ê¸°/ì˜¤íƒˆì ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì˜ˆì‹œ: {example_text}"
            )
            typo_count += 1

    report.append(f"- [LLM] B2B ê¸°ë³¸ê¸°ëŠ¥ì²˜ëŸ¼ ë³´ì´ëŠ” ë¬¸ë‹¨: {b2b_basic_count}ê°œ")
    report.append(f"- [LLM] ë¬´ë£Œ í”„ë¡œëª¨ì…˜ê³¼ B2B íŠœë‹ì´ í˜¼ìš©ëœ ë¬¸ë‹¨: {free_b2b_mix_count}ê°œ")
    report.append(f"- [LLM] í•´ë“œë¦¼ í‘œê¸° ì˜¤í•´ ì†Œì§€ê°€ ìˆëŠ” ë¬¸ë‹¨: {haedream_mislabel_count}ê°œ")
    report.append(f"- [LLM] ë§ì¶¤ë²•/ì˜¤íƒˆì ì§€ì ëœ ë¬¸ë‹¨: {typo_count}ê°œ")


# =========================
# í•œ íŒŒì¼ ì²˜ë¦¬
# =========================

def process_docx(file, filename: str, config: dict, use_llm: bool):
    doc = Document(file)
    report: list[str] = []

    # ë£° ê¸°ë°˜
    check_media_count(doc, report, config["min_images"])
    check_utm_links(doc, report)
    check_hashtags(doc, report, config)
    check_shopby(doc, report, config)
    check_b2b_basic_feature(doc, report, config)
    check_haedream(doc, report, config)
    check_forbidden_terms(doc, report, config["client_brands"], config["competitor_keywords"])
    check_avoided_phrases(doc, report, config["avoided_phrases"])
    check_title_keyword(doc, report, config["title_required_keyword"])

    # LLM ê¸°ë°˜
    check_with_llm(doc, report, config, use_llm)

    # ìš”ì•½
    summary = doc.add_paragraph()
    summary_run = summary.add_run("[ìë™ê²€ìˆ˜ ìš”ì•½]")
    summary_run.bold = True
    for line in report:
        doc.add_paragraph(line)

    buffer = BytesIO()
    doc.save(buffer)
    buffer.seek(0)
    return buffer, report


# =========================
# Streamlit UI + ë¡œê·¸ì¸
# =========================

def main():
    st.set_page_config("ë¸”ë¡œê·¸ ì›ê³  ìë™ê²€ìˆ˜", layout="wide")

    # 1) ë¡œê·¸ì¸ ìœ„ì ¯ ë Œë”
    authenticator.login(
        location="main",
        fields={
            "Form name": "ë¡œê·¸ì¸",
            "Username": "ì•„ì´ë””",
            "Password": "ë¹„ë°€ë²ˆí˜¸",
            "Login": "ë¡œê·¸ì¸",
        },
        key="Login",
    )

    # 2) ì„¸ì…˜ ìƒíƒœì—ì„œ ì¸ì¦ ê²°ê³¼ ì½ê¸°
    auth_status = st.session_state.get("authentication_status", None)
    name = st.session_state.get("name", None)
    username = st.session_state.get("username", None)

    if auth_status is False:
        st.error("ì•„ì´ë”” ë˜ëŠ” ë¹„ë°€ë²ˆí˜¸ê°€ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return
    elif auth_status is None:
        st.info("ì•„ì´ë””ì™€ ë¹„ë°€ë²ˆí˜¸ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        return

    # ë¡œê·¸ì¸ ì„±ê³µ
    authenticator.logout(button_name="ë¡œê·¸ì•„ì›ƒ", location="sidebar", key="Logout")
    st.sidebar.markdown(f"**ğŸ‘¤ {name}ë‹˜ ë¡œê·¸ì¸ ì¤‘**")

    st.title("ğŸ“ ê³ ë„ëª° ë¸”ë¡œê·¸ ì›ê³  ìë™ ê²€ìˆ˜ ë´‡")
    st.markdown("ì›Œë“œ(.docx) ì›ê³ ë¥¼ ì—…ë¡œë“œí•˜ë©´, ì •ì±…ì— ë§ì¶° ìë™ìœ¼ë¡œ í˜•ê´‘íœ + ì½”ë©˜íŠ¸ë¥¼ ë‹¬ì•„ì¤ë‹ˆë‹¤.")

    # ---- ì‚¬ì´ë“œë°”: ê·œì¹™ ì„¤ì • ----
    with st.sidebar:
        st.header("âš™ ê·œì¹™ ì„¤ì •")

        min_images = st.number_input("ìµœì†Œ ì´ë¯¸ì§€ ê°œìˆ˜ ê¸°ì¤€", min_value=0, max_value=100, value=15, step=1)

        hashtags_input = st.text_area(
            "ê¶Œì¥ í•´ì‹œíƒœê·¸ (ì‰¼í‘œë¡œ êµ¬ë¶„)",
            "#ìì‚¬ëª°ì œì‘,#ìì‚¬ëª°ë§Œë“¤ê¸°,#ë¬´ë£Œì‡¼í•‘ëª°ë§Œë“¤ê¸°,#ì˜¨ë¼ì¸ì‡¼í•‘ëª°ì°½ì—…,#B2Bëª°ì œì‘",
            height=70,
        )
        recommended_hashtags = [h.strip() for h in hashtags_input.split(",") if h.strip()]

        b2b_input = st.text_area(
            "B2B ê´€ë ¨ í‚¤ì›Œë“œ",
            "B2B,ë„ë§¤ëª°,ë„ë§¤ ì‡¼í•‘ëª°,íì‡„ëª°,ê°€ë§¹ì  ë°œì£¼,í”„ëœì°¨ì´ì¦ˆ",
            height=60,
        )
        b2b_keywords = [k.strip() for k in b2b_input.split(",") if k.strip()]

        basic_input = st.text_area(
            "â€˜ê¸°ë³¸ ê¸°ëŠ¥â€™ ë‰˜ì•™ìŠ¤ í‚¤ì›Œë“œ",
            "ê¸°ë³¸ ê¸°ëŠ¥,ê¸°ë³¸ê¸°ëŠ¥,ê¸°ë³¸ìœ¼ë¡œ ì œê³µ,ê¸°ë³¸ íƒ‘ì¬,ë³„ë„ ê°œë°œ ì—†ì´,ì¶”ê°€ ê°œë°œ ì—†ì´,ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”",
            height=70,
        )
        basic_feature_keywords = [k.strip() for k in basic_input.split(",") if k.strip()]

        shopby_input = st.text_area(
            "ìƒµë°”ì´ ê´€ë ¨ í‚¤ì›Œë“œ",
            "ìƒµë°”ì´,shopby,Shopby,SHOPBY,ìƒµë°”ì´ ì—”í„°í”„ë¼ì´ì¦ˆ",
            height=60,
        )
        shopby_keywords = [k.strip() for k in shopby_input.split(",") if k.strip()]

        haedream_input = st.text_area(
            "í•´ë“œë¦¼ ê´€ë ¨ í‚¤ì›Œë“œ",
            "í•´ë“œë¦¼,í—¤ë“œë¦¼",
            height=50,
        )
        haedream_keywords = [k.strip() for k in haedream_input.split(",") if k.strip()]

        client_brands_input = st.text_area(
            "ê³ ê°ì‚¬ ë¸Œëœë“œëª… (ì–¸ê¸‰ ê¸ˆì§€, ì‰¼í‘œë¡œ)",
            "ê³ ê°A,ê³ ê°B",
            height=60,
        )
        client_brands = [c.strip() for c in client_brands_input.split(",") if c.strip()]

        competitors_input = st.text_area(
            "íƒ€ì‚¬/ê²½ìŸì‚¬ í‚¤ì›Œë“œ (ì–¸ê¸‰ ê¸ˆì§€, ì‰¼í‘œë¡œ)",
            "ì¹´í˜24,ì•„ì„ì›¹,ë©”ì´í¬ìƒµ,shopify",
            height=60,
        )
        competitor_keywords = [c.strip() for c in competitors_input.split(",") if c.strip()]

        avoided_input = st.text_area(
            "ì§€ì–‘ í‘œí˜„ ë¦¬ìŠ¤íŠ¸ (ì‡¼í•‘ëª°í˜¸ìŠ¤íŒ…ì‚¬, ì „ììƒê±°ë˜ í”Œë«í¼ ë“±)",
            "ì‡¼í•‘ëª°í˜¸ìŠ¤íŒ…ì‚¬,ì‡¼í•‘ëª° í˜¸ìŠ¤íŒ…ì‚¬,ì „ììƒê±°ë˜ í”Œë«í¼,ë°˜ì‘í˜• ìŠ¤í‚¨,ë°˜ì‘í˜•ìŠ¤í‚¨",
            height=70,
        )
        avoided_phrases = [p.strip() for p in avoided_input.split(",") if p.strip()]

        title_required_keyword = st.text_input(
            "ì œëª©ì— ë°˜ë“œì‹œ ë“¤ì–´ê°€ì•¼ í•  í‚¤ì›Œë“œ (ì—†ìœ¼ë©´ ë¹„ì›Œë‘ê¸°)",
            "",
        )

        suspicious_input = st.text_area(
            "LLM ê²€ìˆ˜ ëŒ€ìƒ 'ì˜ì‹¬ í‚¤ì›Œë“œ'",
            "B2B,ë„ë§¤ëª°,íì‡„ëª°,í”„ëœì°¨ì´ì¦ˆ,ê°€ë§¹ì ,ë¬´ë£Œ,0ì›,í”„ë¡œëª¨ì…˜,ë¬´ìƒ,í•´ë“œë¦¼,í—¤ë“œë¦¼",
            height=70,
        )
        suspicious_keywords = [k.strip() for k in suspicious_input.split(",") if k.strip()]

        use_llm = st.checkbox("LLM ê¸°ë°˜ ë‰˜ì•™ìŠ¤/ë§ì¶¤ë²• ê²€ìˆ˜ ì‚¬ìš©", value=True)

        config = {
            "min_images": int(min_images),
            "recommended_hashtags": recommended_hashtags,
            "b2b_keywords": b2b_keywords,
            "basic_feature_keywords": basic_feature_keywords,
            "shopby_keywords": shopby_keywords,
            "haedream_keywords": haedream_keywords,
            "client_brands": client_brands,
            "competitor_keywords": competitor_keywords,
            "avoided_phrases": avoided_phrases,
            "title_required_keyword": title_required_keyword.strip() or None,
            "suspicious_keywords": suspicious_keywords,
        }

    # ---- íŒŒì¼ ì—…ë¡œë“œ ----
    uploaded_files = st.file_uploader(
        "ê²€ìˆ˜í•  ì›Œë“œ íŒŒì¼(.docx)ì„ ì—…ë¡œë“œí•˜ì„¸ìš” (ì—¬ëŸ¬ ê°œ ê°€ëŠ¥)",
        type=["docx"],
        accept_multiple_files=True,
    )

    if uploaded_files:
        for file in uploaded_files:
            st.subheader(f"ğŸ“„ {file.name}")
            buffer, report = process_docx(file, file.name, config, use_llm)

            st.text_area("ê²€ìˆ˜ ìš”ì•½", "\n".join(report), height=180)

            st.download_button(
                "âœ… ê²€ìˆ˜ëœ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                data=buffer,
                file_name=file.name.replace(".docx", "_checked.docx"),
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            )


if __name__ == "__main__":
    main()
